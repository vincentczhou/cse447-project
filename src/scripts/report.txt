CSE 447 - Checkpoint 3 Report

=== Thoughtful Use of AI ===

If we didn't know how to code, we could still accomplish this next-character prediction task effectively using existing LLMs through their chat or API interfaces. The approach would be to craft a prompt that presents each text prefix to the model and asks it to predict the most likely next character. For example, we could use ChatGPT, Gemini, or a similar service and provide a system prompt like: "You are a next-character predictor. Given a text prefix, respond with exactly 3 characters that are most likely to follow, in order of probability, with no spaces or explanation." We would then feed each line from the input file as a user message and collect the model's 3-character response as our prediction. To handle the full dataset efficiently without writing a full program, we could batch all inputs into a single numbered list within one prompt and ask the model to reply with one line of 3 characters per input—this keeps costs low and avoids rate limits. The key insight is that large language models already have deep knowledge of character-level patterns across many languages, since they are trained on massive multilingual corpora. Their token-level predictions implicitly encode strong character-level priors—especially for common languages like English—making them well-suited for this task even without any task-specific fine-tuning.


=== Benchmarking ===

To benchmark an LLM-based approach, we evaluated Google's Gemini 2.5 Flash via the Gemini API on two datasets: the provided example data (13 English inputs from example/input.txt) and a random sample of 500 inputs from our held-out multilingual validation set (7,840 total inputs from our MADLAD-based multilingual dataset). We batched all inputs into chunks of 50 per API call and used JSON structured output (response_mime_type="application/json" with an array-of-strings schema) so the model returns a machine-parseable JSON array rather than free-form text. Thinking was disabled (thinking_budget=0) to maximize output token budget and reduce latency. On the small example dataset (13 well-known English phrases), Gemini 2.5 Flash achieved a perfect 100% success rate (13/13) in 0.91 seconds total (69.8ms per input, 14.3 inputs/s). On the larger 500-input multilingual sample (English plus other languages from the MADLAD corpus), accuracy was 46.2% (231/500) over 22.11 seconds (44.2ms per input, 22.6 inputs/s across 10 API calls).

Notably, switching from plain-text output to JSON structured output improved validation accuracy from 29.2% to 46.2% (+17 percentage points). The plain-text approach suffered from formatting inconsistencies—the model sometimes returned line numbers, extra whitespace, or merged lines—causing many predictions to be lost or misparsed. Structured output eliminates this class of errors entirely by constraining the model to return a valid JSON array. The remaining accuracy gap between curated English phrases (100%) and the multilingual validation set (46.2%) reflects the inherent difficulty of predicting next characters in diverse, noisy, multilingual web-scraped text (URLs, special characters, and non-Latin scripts). Additionally, the LLM approach has practical limitations: even with aggressive batching (50 inputs per API call), processing the full 7,840-input validation set would require ~157 API calls and take roughly 3–4 minutes, compared to under 30 seconds for our local KenLM model. Our KenLM-based system, by contrast, is deterministic, runs entirely locally, processes inputs in parallel with multiprocessing, and has zero cost per query. The LLM approach could be improved by using a locally-hosted model to eliminate API latency, or by using LLM predictions as a reranking signal on top of the n-gram model's candidates.
