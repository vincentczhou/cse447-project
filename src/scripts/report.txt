CSE 447 - Checkpoint 3 Report

=== Thoughtful Use of AI ===

If we didn't know how to code, we could still accomplish this next-character prediction task effectively using existing LLMs through their chat or API interfaces. The approach would be to craft a prompt that presents each text prefix to the model and asks it to predict the most likely next character. For example, we could use ChatGPT, Gemini, or a similar service and provide a system prompt like: "You are a next-character predictor. Given a text prefix, respond with exactly 3 characters that are most likely to follow, in order of probability, with no spaces or explanation." We would then feed each line from the input file as a user message and collect the model's 3-character response as our prediction. To handle the full dataset efficiently without writing a full program, we could batch all inputs into a single numbered list within one prompt and ask the model to reply with one line of 3 characters per input—this keeps costs low and avoids rate limits. The key insight is that large language models already have deep knowledge of character-level patterns across many languages, since they are trained on massive multilingual corpora. Their token-level predictions implicitly encode strong character-level priors—especially for common languages like English—making them well-suited for this task even without any task-specific fine-tuning.


=== Benchmarking ===

To benchmark an LLM-based approach, we evaluated Google's Gemini 2.5 Flash via the Gemini API on two datasets: the provided example data (13 English inputs from example/input.txt) and a random sample of 500 inputs from our held-out multilingual validation set (7,840 total inputs from our MADLAD-based multilingual dataset). We iterated through three configurations to understand how output structure and request granularity affect accuracy:

1. Batched plain-text output (50 inputs per API call, free-form text response): 29.2% accuracy (146/500), 58.5ms per input. The model frequently returned line numbers, extra whitespace, or merged lines, causing many predictions to be misparsed.

2. Batched JSON structured output (50 inputs per API call, response_mime_type="application/json" with an array-of-strings schema): 46.2% accuracy (231/500), 44.2ms per input. Structured output eliminated formatting errors, yielding a +17pp improvement over plain text.

3. Single-request structured JSON output (1 input per API call, response schema with explicit "first", "second", "third" fields for each character): 79.0% accuracy (395/500), 700.7ms per input. Giving the model a single input to focus on, combined with a structured object schema that forces exactly one character per field, produced the best results by far.

All configurations used thinking_budget=0 and temperature=0.0. On the small example dataset (13 well-known English phrases), all three approaches achieved 100% accuracy (13/13).

The progression from 29.2% → 46.2% → 79.0% demonstrates two key findings: (a) structured output schemas dramatically reduce parsing errors and give the model clearer constraints, and (b) single-input requests allow the model to devote its full attention to each prediction rather than splitting focus across a batch. The tradeoff is latency—the single-request approach takes ~700ms per input (1.4 inputs/s) compared to ~44ms per input with batching (22.6 inputs/s). Processing the full 7,840-input validation set at single-request speed would take approximately 90 minutes, compared to under 30 seconds for our local KenLM model. Our KenLM-based system, by contrast, is deterministic, runs entirely locally, processes inputs in parallel with multiprocessing, and has zero cost per query—making it far more practical for the submission requirements despite lower accuracy on some inputs. The LLM approach could be improved further by using a locally-hosted model to eliminate API latency, or by combining LLM predictions as a reranking signal on top of the n-gram model's candidates.
